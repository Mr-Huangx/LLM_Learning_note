# LLM_Learning_note

这是一个关于大语言模型（LLM）和Transformer相关知识的学习笔记仓库，用于记录和总结学习过程中的重点内容。

## 项目结构

```
├── LLM/                    # 大语言模型相关学习笔记
│   └── LLM.md            # LLM核心知识总结
└── Transformer基础知识/     # Transformer架构学习资料
    ├── 1、attention及其变体.md    # Attention机制及其变体详解
    ├── 2、Encoder-Decoder.md      # Encoder-Decoder架构介绍
    └── 3、预训练语言模型.md        # 预训练语言模型原理与应用
```

## 内容概述

### Transformer基础知识
1. **Attention及其变体**：详细介绍了注意力机制的基本原理、各种变体（如Self-Attention、Multi-Head Attention等）及其在NLP中的应用。
2. **Encoder-Decoder**：讲解了经典的Encoder-Decoder架构，以及Transformer如何在此基础上进行改进。
3. **预训练语言模型**：探讨了预训练语言模型的发展历程、主要技术（如BERT、GPT等）及其应用场景。

### LLM
- 深入学习大语言模型的核心原理、训练方法、应用场景和最新研究进展。

## 学习计划

1. 掌握Transformer架构的基本原理和核心组件
2. 深入理解各种Attention机制的工作原理
3. 学习预训练语言模型的发展和应用
4. 研究大语言模型的最新技术和挑战

## 参考资料

学习过程中参考了大量论文、教程和技术博客，主要包括：

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Transformer架构的奠基性论文

- [万字长文带你梳理Llama开源家族：从Llama-1到Llama-3](https://mp.weixin.qq.com/s/5_VnzP3JmOB0D5geV5HRFg) - 张帆, 陈安东
- [An Intuition for Attention](https://jaykmody.com/blog/attention-intuition/) - Jay Mody
- [细节拉满，全网最详细的Transformer介绍（含大量插图）](https://zhuanlan.zhihu.com/p/644122223) - 知乎

### 开源项目
- [Happy-LLM](https://github.com/Happy-LLM/Happy-LLM) - 一个开源的大语言模型学习和实践项目


## 说明

本仓库仅用于个人学习和知识整理，如有错误或不足之处，欢迎指正。