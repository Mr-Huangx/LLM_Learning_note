
# attention
æºè‡ªCVé¢†åŸŸï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šç”¨æˆ·åœ¨æ‹¿åˆ°ä¸€å¼ å›¾ç‰‡æ—¶ï¼Œä»–å¯¹æ‰€æœ‰çš„åƒç´ çš„å…³æ³¨åº¦åº”è¯¥æ˜¯ä¸ä¸€æ ·çš„ï¼Œä¾‹å¦‚ä»–æ‹¿åˆ°ä¸€å¼ å›¾ç‰‡ï¼Œä»–æƒ³è¦å¯»æ‰¾å›¾ç‰‡ä¸­çŒ«å’ªçš„ä¿¡æ¯ã€‚å› æ­¤ï¼Œä»–åº”è¯¥æ›´åŠ å…³æ³¨çŒ«å’ªç›¸å…³çš„åƒç´ ï¼Œè€Œå¯¹å…¶ä»–æ— å…³çš„åƒç´ ä¸å…³å¿ƒã€‚

å¼•å…¥åˆ°NLPé‡Œé¢ï¼Œå³ç”¨æˆ·æ‹¿åˆ°ä¸€ä¸ªæ–‡æœ¬åºåˆ—ï¼Œä»–å¯¹æ‰€æœ‰tokençš„å…³æ³¨åº¦åº”è¯¥ä¹Ÿæ˜¯ä¸ä¸€æ ·çš„ï¼Œä¾‹å¦‚ä»–å…³å¿ƒæ–‡æœ¬çš„å‘å¸ƒæ—¶é—´ï¼Œé‚£ä»–åº”è¯¥å¯¹å…¶ä¸­æ—¶é—´ç›¸å…³çš„ä¿¡æ¯åˆ†é…æ›´å¤§çš„æƒé‡ã€‚

ä»æ•°å­¦è®¡ç®—ä¸­ï¼Œå…¶æœ¬èº«å°±æ˜¯ä¸€ä¸ªqã€kã€vçš„è®¡ç®—ã€‚é€šè¿‡queryå’Œkeyè¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—ï¼Œå¾—åˆ°å¯¹vçš„æƒé‡ï¼Œç„¶åå¯¹vè¿›è¡ŒåŠ æƒæ±‚å’Œã€‚

---
## ä»£ç å®ç°
```python
import torch
import math

def attention(query, key, value, dropout=None):
    dim = query.shape[-1]

    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(dim)
    att_score = scores.softmax(dim=-1)

    if dropout is not None:
        att_score = dropout(att_score)

    return torch.matmul(att_score, value), att_score
```
---

# self-attention
**self-attention** æ˜¯è®©åºåˆ—ä¸­çš„æ¯ä¸ª tokenï¼ˆè¯ï¼‰éƒ½èƒ½â€œçœ‹è§â€æ•´ä¸ªå¥å­ï¼Œå¹¶æ ¹æ®æ‰€æœ‰è¯è®¡ç®—å‡ºè‡ªå·±çš„â€œä¸Šä¸‹æ–‡è¡¨ç¤ºâ€ã€‚

> æ›´ç›´ç™½çš„æ˜¯ï¼šattentionçš„queryã€keyå’Œvalueä¸€èˆ¬æ¥è‡ªä¸åŒçš„è¾“å…¥ï¼Œè€Œself-attentionæ˜¯æ¥æºäºåŒä¸€è¾“å…¥ï¼Œè¿›è¡Œä¸åŒçš„çº¿æ€§å˜åŒ–è·å¾—ã€‚
>

---

ä¸¾ä¸ªä¾‹å­ï¼š

> å¥å­ï¼š**I love playing football**
>

å¯¹äºå•è¯ â€œloveâ€ï¼Œ  
å®ƒä¼šâ€œæ³¨æ„â€åˆ°ï¼š

+ â€œIâ€ â†’ ä¸»è¯­æ˜¯è°
+ â€œplaying footballâ€ â†’ åŠ¨ä½œå’Œå¯¹è±¡

è¿™æ ·æ¨¡å‹èƒ½ç†è§£ â€œloveâ€ çš„è¯­ä¹‰

---
## ä»£ç å®ç°
```python
# attention ä¸ºä¸Šæ–‡å®šä¹‰çš„æ³¨æ„åŠ›è®¡ç®—å‡½æ•°
attention(x, x, x)
```

---

# mask-attention
åœ¨æŸäº›ä»»åŠ¡ä¸­ï¼ˆæ¯”å¦‚ GPT çš„æ–‡æœ¬ç”Ÿæˆï¼‰ï¼Œæˆ‘ä»¬å¸Œæœ›æ¨¡å‹ï¼š

> åªèƒ½çœ‹åˆ°**ä¹‹å‰çš„è¯**ï¼Œ  
ä¸èƒ½çœ‹åˆ°**æœªæ¥çš„è¯**ï¼
>

å¦åˆ™ï¼Œè®­ç»ƒæ—¶æ¨¡å‹å°±ä¼šâ€œå·çœ‹ç­”æ¡ˆâ€ã€‚**ä¸æ­¤åŒæ—¶**ï¼Œmask-attentionå¯ä»¥è§£å†³self-attentionä¸å¯å¹¶è¡Œå¤„ç†çš„é—®é¢˜ã€‚

---

##  ä¸¾ä¾‹
å‡è®¾æˆ‘ä»¬æœ‰ä¸€å¥è¯ï¼š

> â€œI love catsâ€
>

æ¨¡å‹åœ¨é¢„æµ‹ç¬¬ä¸‰ä¸ªè¯ â€œcatsâ€ æ—¶ï¼š

+ âœ… å¯ä»¥çœ‹åˆ° â€œIâ€ å’Œ â€œloveâ€
+ âŒ ä¸å¯ä»¥çœ‹åˆ° â€œcatsâ€ è‡ªå·±ä¹‹åçš„è¯ï¼ˆå› ä¸ºæœªæ¥çš„ä¿¡æ¯ä¸èƒ½ç”¨ï¼‰

è¿™å°±æ˜¯ **â€œmaskingï¼ˆé®æŒ¡ï¼‰â€** çš„ç›®çš„ã€‚

## Mask Self-Attention çš„å®ç°åŸç†
åœ¨å®é™…çš„åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›æ¨¡å‹é€šè¿‡å†å²çš„æ•°æ®æ¥é¢„æµ‹æœªæ¥çš„ä¿¡æ¯ï¼Œå› æ­¤åœ¨åˆ†é…æƒé‡çš„æ—¶å€™ï¼Œéœ€è¦æŠŠæœªæ¥çš„ä¿¡æ¯è¿›è¡Œé®è”½ï¼Œè¿™ç§é®è”½å«åšmaskã€‚
ç»“åˆmaskå’Œattentionï¼Œå°±å«åšæ©ç æ³¨æ„åŠ›æœºåˆ¶ã€‚  
åœ¨ self-attention é‡Œï¼Œæˆ‘ä»¬ä¼šè®¡ç®—ä¸€ä¸ªæ³¨æ„åŠ›çŸ©é˜µï¼ˆattention matrixï¼‰ï¼Œ  
å½¢çŠ¶é€šå¸¸æ˜¯ `[seq_len, seq_len]`ã€‚

æ¯”å¦‚å¥å­é•¿åº¦ä¸º 4ï¼Œå°±ä¼šç”Ÿæˆ 4Ã—4 çš„çŸ©é˜µï¼Œ  
å…¶ä¸­ç¬¬ i è¡Œç¬¬ j åˆ—è¡¨ç¤ºç¬¬ i ä¸ªè¯å¯¹ç¬¬ j ä¸ªè¯çš„æ³¨æ„åŠ›æƒé‡ã€‚

### åŠ  Mask ä¹‹å‰ï¼š
æ¯ä¸ªè¯å¯ä»¥å…³æ³¨æ‰€æœ‰è¯ï¼ˆåŒ…æ‹¬æœªæ¥çš„ï¼‰

| è¯ | I | love | cats | very |
| --- | --- | --- | --- | --- |
| I | âœ… | âœ… | âœ… | âœ… |
| love | âœ… | âœ… | âœ… | âœ… |
| cats | âœ… | âœ… | âœ… | âœ… |
| very | âœ… | âœ… | âœ… | âœ… |


---

### åŠ  Mask ä¹‹åï¼ˆä¸‹ä¸‰è§’çŸ©é˜µï¼‰ï¼š
åªèƒ½å…³æ³¨è‡ªå·±**ä¹‹å‰**çš„è¯

| è¯ | I | love | cats | very |
| --- | --- | --- | --- | --- |
| I | âœ… | âŒ | âŒ | âŒ |
| love | âœ… | âœ… | âŒ | âŒ |
| cats | âœ… | âœ… | âœ… | âŒ |
| very | âœ… | âœ… | âœ… | âœ… |


è¿™ä¸ªâ€œâŒâ€å¯¹åº”çš„åœ°æ–¹ä¼šè¢« mask æˆ **è´Ÿæ— ç©·ï¼ˆâˆ’âˆï¼‰**ï¼Œ  
è¿™æ ·åœ¨ softmax ä¹‹åï¼Œå®ƒçš„æ³¨æ„åŠ›æƒé‡å°±æ˜¯ 0ã€‚

---
## ä»£ç 
```python
# åˆ›å»ºä¸€ä¸ªä¸Šä¸‰è§’çŸ©é˜µï¼Œç”¨äºé®è”½æœªæ¥ä¿¡æ¯ã€‚
# å…ˆé€šè¿‡ full å‡½æ•°åˆ›å»ºä¸€ä¸ª 1 * seq_len * seq_len çš„çŸ©é˜µ
import torch

max_seq_len = 512
mask = torch.full((1, max_seq_len, max_seq_len), float("-inf"))
# triu å‡½æ•°çš„åŠŸèƒ½æ˜¯åˆ›å»ºä¸€ä¸ªä¸Šä¸‰è§’çŸ©é˜µ
mask = torch.triu(mask, diagonal=1)

# æ­¤å¤„çš„ scores ä¸ºè®¡ç®—å¾—åˆ°çš„æ³¨æ„åŠ›åˆ†æ•°(QK^T/dim^(1/2))ï¼Œmask ä¸ºä¸Šæ–‡ç”Ÿæˆçš„æ©ç çŸ©é˜µ
scores = scores + mask[:, :max_seq_len, :max_seq_len]
scores = F.softmax(scores.float(), dim=-1).type_as(xq)
```
---

# Multi-Head Attention çš„æ€æƒ³
Transformer çš„ä½œè€…æƒ³åˆ°ï¼š

> æ—¢ç„¶ä¸€ä¸ªæ³¨æ„åŠ›å¤´åªèƒ½å…³æ³¨ä¸€ç§å…³ç³»ï¼Œ  
é‚£æˆ‘å¯ä»¥è®©æ¨¡å‹â€œé•¿å¤šä¸ªè„‘è¢‹â€æ¥**å…³æ³¨ä¸åŒçš„å…³ç³»**ï¼
>

## ä¸¾ä¸ªä¾‹å­
å‡è®¾ä½ æœ‰ä¸€å¥è¯ï¼š

> â€œThe cat sat on the mat.â€
>

Transformer å¯èƒ½æœ‰ï¼š

+ ğŸ§© **Head 1**ï¼šå…³æ³¨ â€œcatâ€ å’Œ â€œsatâ€ çš„å…³ç³»ï¼ˆä¸»è¯­-åŠ¨è¯ï¼‰
+ ğŸ§© **Head 2**ï¼šå…³æ³¨ â€œonâ€ å’Œ â€œmatâ€çš„å…³ç³»ï¼ˆä»‹è¯-å®¾è¯­ï¼‰
+ ğŸ§© **Head 3**ï¼šå…³æ³¨ â€œtheâ€ å’Œ â€œåè¯â€çš„ä¾é™„å…³ç³»

æœ€åæŠŠè¿™äº›ä¸åŒâ€œè§†è§’â€çš„ä¿¡æ¯åˆå¹¶ï¼Œå¾—åˆ°ä¸€ä¸ªæ›´ä¸°å¯Œçš„è¯­ä¹‰è¡¨ç¤ºã€‚

## ç»“æ„åŸç†ï¼ˆæ•°å­¦è§’åº¦ï¼‰
### 1ï¸âƒ£ è¾“å…¥ï¼š
å‡è®¾è¾“å…¥å‘é‡æ˜¯ $ X\in \mathbb{R}^{n \times d_{model}} $ï¼Œ  
å…¶ä¸­ï¼š

+ $ n $ï¼šåºåˆ—é•¿åº¦
+ $ X\in \mathbb{R}^{n \times d_{model}} $ï¼šåµŒå…¥ç»´åº¦ï¼ˆä¾‹å¦‚ 512ï¼‰

---

### 2ï¸âƒ£ å¤šå¤´æ‹†åˆ†ï¼š
æˆ‘ä»¬å®šä¹‰ **h ä¸ªæ³¨æ„åŠ›å¤´ï¼ˆheadsï¼‰**ï¼Œ  
æ¯ä¸ªå¤´éƒ½æœ‰è‡ªå·±çš„ä¸€ç»„æƒé‡ï¼š

$ Q_iâ€‹=XW_i^Qâ€‹,K_iâ€‹=XW_i^Kâ€‹,V_iâ€‹=XW_i^Vâ€‹ $

å…¶ä¸­$ Q_iâ€‹â€‹,K_i,V_iâ€‹ $  éƒ½æ˜¯å¯å­¦ä¹ å‚æ•°ã€‚$ W_i $çš„çº¬åº¦é€šå¸¸ä¸emb_dimä¸åŒã€‚å¯ä»¥ç†è§£æ˜¯å°†$ X $çš„è¡¨å¾ä¸­çš„å­ç©ºé—´è¿›è¡Œæå–ï¼Œå³å½“å‰çš„attentionåªå…³æ³¨æŸä¸ªå­ç©ºé—´çš„ç›¸å…³æ€§ï¼Œè€Œä¸æ˜¯æ•´ä¸ªtoken embeddingç©ºé—´ä¸­çš„ç›¸å…³æ€§ã€‚

---

### 3ï¸âƒ£ æ¯ä¸ªå¤´ç‹¬ç«‹è®¡ç®— Attentionï¼š
$ head_i=Attention(Q_i,K_i,V_i) $

---

### 4ï¸âƒ£ æ‹¼æ¥ + çº¿æ€§å˜æ¢ï¼š
$ MultiHead(X)=Concat(head_1,â€¦,head_h)W^O $

æœ€ç»ˆå¾—åˆ°å’ŒåŸå§‹ç»´åº¦ä¸€æ ·çš„è¾“å‡ºã€‚$ W^o\in \mathbb{R}^{d_{model} \times d_{model}} $ï¼Œåªæ˜¯åšçº¿æ€§å˜æ¢ã€‚


---
## ä»£ç å®ç°
å…¶æœ€ç›´è§‚çš„ä»£ç å®ç°å¹¶ä¸å¤æ‚ï¼Œå³ n ä¸ªå¤´å°±æœ‰ n ç»„3ä¸ªå‚æ•°çŸ©é˜µï¼Œæ¯ä¸€ç»„è¿›è¡ŒåŒæ ·çš„æ³¨æ„åŠ›è®¡ç®—ï¼Œä½†ç”±äºæ˜¯ä¸åŒçš„å‚æ•°çŸ©é˜µä»è€Œé€šè¿‡åå‘ä¼ æ’­å®ç°äº†ä¸åŒçš„æ³¨æ„åŠ›ç»“æœï¼Œç„¶åå°† n ä¸ªç»“æœæ‹¼æ¥èµ·æ¥è¾“å‡ºå³å¯ã€‚

ä½†ä¸Šè¿°å®ç°æ—¶ç©ºå¤æ‚åº¦å‡è¾ƒé«˜ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡çŸ©é˜µè¿ç®—å·§å¦™åœ°å®ç°å¹¶è¡Œçš„å¤šå¤´è®¡ç®—ï¼Œå…¶æ ¸å¿ƒé€»è¾‘åœ¨äºä½¿ç”¨ä¸‰ä¸ªç»„åˆçŸ©é˜µæ¥ä»£æ›¿äº†nä¸ªå‚æ•°çŸ©é˜µçš„ç»„åˆï¼Œä¹Ÿå°±æ˜¯çŸ©é˜µå†…ç§¯å†æ‹¼æ¥å…¶å®ç­‰åŒäºæ‹¼æ¥çŸ©é˜µå†å†…ç§¯ã€‚å…·ä½“å®ç°å¯ä»¥å‚è€ƒä¸‹åˆ—ä»£ç :

```python
import torch.nn as nn
import torch

'''å¤šå¤´è‡ªæ³¨æ„åŠ›è®¡ç®—æ¨¡å—'''
class MultiHeadAttention(nn.Module):

    def __init__(self, args: ModelArgs, is_causal=False):
        # æ„é€ å‡½æ•°
        # args: é…ç½®å¯¹è±¡
        super().__init__()
        # éšè—å±‚ç»´åº¦å¿…é¡»æ˜¯å¤´æ•°çš„æ•´æ•°å€ï¼Œå› ä¸ºåé¢æˆ‘ä»¬ä¼šå°†è¾“å…¥æ‹†æˆå¤´æ•°ä¸ªçŸ©é˜µ
        assert args.dim % args.n_heads == 0
        # æ¯ä¸ªå¤´çš„ç»´åº¦ï¼Œç­‰äºæ¨¡å‹ç»´åº¦é™¤ä»¥å¤´çš„æ€»æ•°ã€‚
        self.head_dim = args.dim // args.n_heads
        self.n_heads = args.n_heads

        # Wq, Wk, Wv å‚æ•°çŸ©é˜µï¼Œæ¯ä¸ªå‚æ•°çŸ©é˜µä¸º n_embd x dim
        # è¿™é‡Œé€šè¿‡ä¸‰ä¸ªç»„åˆçŸ©é˜µæ¥ä»£æ›¿äº†nä¸ªå‚æ•°çŸ©é˜µçš„ç»„åˆï¼Œå…¶é€»è¾‘åœ¨äºçŸ©é˜µå†…ç§¯å†æ‹¼æ¥å…¶å®ç­‰åŒäºæ‹¼æ¥çŸ©é˜µå†å†…ç§¯ï¼Œ
        self.wq = nn.Linear(args.n_embd, self.n_heads * self.head_dim, bias=False)
        self.wk = nn.Linear(args.n_embd, self.n_heads * self.head_dim, bias=False)
        self.wv = nn.Linear(args.n_embd, self.n_heads * self.head_dim, bias=False)
        # è¾“å‡ºæƒé‡çŸ©é˜µï¼Œç»´åº¦ä¸º dim x dimï¼ˆhead_dim = dim / n_headsï¼‰
        self.wo = nn.Linear(self.n_heads * self.head_dim, args.dim, bias=False)
        # æ³¨æ„åŠ›çš„ dropout
        self.attn_dropout = nn.Dropout(args.dropout)
        # æ®‹å·®è¿æ¥çš„ dropout
        self.resid_dropout = nn.Dropout(args.dropout)
        self.is_causal = is_causal

        # åˆ›å»ºä¸€ä¸ªä¸Šä¸‰è§’çŸ©é˜µï¼Œç”¨äºé®è”½æœªæ¥ä¿¡æ¯
        # æ³¨æ„ï¼Œå› ä¸ºæ˜¯å¤šå¤´æ³¨æ„åŠ›ï¼ŒMask çŸ©é˜µæ¯”ä¹‹å‰æˆ‘ä»¬å®šä¹‰çš„å¤šä¸€ä¸ªç»´åº¦
        if is_causal:
            mask = torch.full((1, 1, args.max_seq_len, args.max_seq_len), float("-inf"))
            mask = torch.triu(mask, diagonal=1)
            # æ³¨å†Œä¸ºæ¨¡å‹çš„ç¼“å†²åŒº
            self.register_buffer("mask", mask)

    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):

        # è·å–æ‰¹æ¬¡å¤§å°å’Œåºåˆ—é•¿åº¦ï¼Œ[batch_size, seq_len, dim]
        bsz, seqlen, _ = q.shape

        # è®¡ç®—æŸ¥è¯¢ï¼ˆQï¼‰ã€é”®ï¼ˆKï¼‰ã€å€¼ï¼ˆVï¼‰,è¾“å…¥é€šè¿‡å‚æ•°çŸ©é˜µå±‚ï¼Œç»´åº¦ä¸º (B, T, n_embed) x (n_embed, dim) -> (B, T, dim)
        xq, xk, xv = self.wq(q), self.wk(k), self.wv(v)

        # å°† Qã€Kã€V æ‹†åˆ†æˆå¤šå¤´ï¼Œç»´åº¦ä¸º (B, T, n_head, dim // n_head)ï¼Œç„¶åäº¤æ¢ç»´åº¦ï¼Œå˜æˆ (B, n_head, T, dim // n_head)
        # å› ä¸ºåœ¨æ³¨æ„åŠ›è®¡ç®—ä¸­æˆ‘ä»¬æ˜¯å–äº†åä¸¤ä¸ªç»´åº¦å‚ä¸è®¡ç®—
        xq = xq.view(bsz, seqlen, self.n_heads, self.head_dim)
        xk = xk.view(bsz, seqlen, self.n_heads, self.head_dim)
        xv = xv.view(bsz, seqlen, self.n_heads, self.head_dim)
        xq = xq.transpose(1, 2)
        xk = xk.transpose(1, 2)
        xv = xv.transpose(1, 2)

        # æ³¨æ„åŠ›è®¡ç®—
        # è®¡ç®— QK^T / sqrt(d_k)ï¼Œç»´åº¦ä¸º (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)
        scores = torch.matmul(xq, xk.transpose(2, 3)) / math.sqrt(self.head_dim)
        # æ©ç è‡ªæ³¨æ„åŠ›å¿…é¡»æœ‰æ³¨æ„åŠ›æ©ç 
        if self.is_causal:
            assert hasattr(self, 'mask')
            # è¿™é‡Œæˆªå–åˆ°åºåˆ—é•¿åº¦ï¼Œå› ä¸ºæœ‰äº›åºåˆ—å¯èƒ½æ¯” max_seq_len çŸ­
            scores = scores + self.mask[:, :, :seqlen, :seqlen]
        # è®¡ç®— softmaxï¼Œç»´åº¦ä¸º (B, nh, T, T)
        scores = F.softmax(scores.float(), dim=-1).type_as(xq)
        # åš Dropout
        scores = self.attn_dropout(scores)
        # V * Scoreï¼Œç»´åº¦ä¸º(B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)
        output = torch.matmul(scores, xv)

        # æ¢å¤æ—¶é—´ç»´åº¦å¹¶åˆå¹¶å¤´ã€‚
        # å°†å¤šå¤´çš„ç»“æœæ‹¼æ¥èµ·æ¥, å…ˆäº¤æ¢ç»´åº¦ä¸º (B, T, n_head, dim // n_head)ï¼Œå†æ‹¼æ¥æˆ (B, T, n_head * dim // n_head)
        # contiguous å‡½æ•°ç”¨äºé‡æ–°å¼€è¾Ÿä¸€å—æ–°å†…å­˜å­˜å‚¨ï¼Œå› ä¸ºPytorchè®¾ç½®å…ˆtransposeå†viewä¼šæŠ¥é”™ï¼Œ
        # å› ä¸ºviewç›´æ¥åŸºäºåº•å±‚å­˜å‚¨å¾—åˆ°ï¼Œç„¶è€Œtransposeå¹¶ä¸ä¼šæ”¹å˜åº•å±‚å­˜å‚¨ï¼Œå› æ­¤éœ€è¦é¢å¤–å­˜å‚¨
        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)

        # æœ€ç»ˆæŠ•å½±å›æ®‹å·®æµã€‚
        output = self.wo(output)
        output = self.resid_dropout(output)
        return output

```

