Pre-Train language Model是有ELMo（embedding from Language Model）所提出的训练思路。针对Encoer、Decoder的特点，研究人员开始对Transformer进行了不同的优化。

#  Encoder-only PLM
## BERT
BERT，全名为 Bidirectional Encoder Representations from Transformers，是由 Google 团队在 2018年发布的预训练语言模型。BERT开创了预训练+微调的自然语言处理任务的新模式。

### 思想延承
BERT所继承的核心思想包括：

+ Transformer架构。
+ 预训练+微调范式。



### 模型架构-Encoder Only
BERT 的模型架构是取了 Transformer 的 Encoder 部分堆叠而成，如下图所示（下图是改进版的，非原文模型）：

Bert架构图

BERT是针对于 NLU （Natural Language Understanding，NLU）任务打造的预训练模型，其输入一般是文本序列，如果不做特殊处理，则其Encoder输出的是文本序列的高维表征，为了进行训练，构造成Seq2Seq的模型，使其能够适配各种NLU任务，在BERT顶层设计了一个prediction_heads的模块，将高维的表征通过线性层转换成分类纬度。（例如，如果一共有2个类别，则输出为两纬度向量）。

模型整体既是由Embedding、Encoder加上prediction_heads组成：

![](https://cdn.nlark.com/yuque/0/2025/png/35251293/1764679985531-87c0945c-e934-440e-b9ec-cfc5843525ff.png)

BERT简略图

BERT在注意力计算过程中与Transformer的差异在于，完成注意力分数的计算后，先通过Position Embedding层融入相对位置信息，这里的Position Embedding层是一层线性矩阵。可以通过可训练的参数来拟合相对位置，相对而言Transformer使用的绝对位置编码sin、cos能够拟合更多的相对位置信息。But，同时也带来了不少的模型参数，无法处理超过模型训练长度的输入。

### 预训练任务-MLM+NSP
前面两小节是对于模型思想、模型结构的介绍，下面介绍的是如何进行训练。BERT提出了两个新的预训练任务--MLM（Mask Language Model）和NSP（Next Sentence Prediction）。

#### MLM（Mask Language Model）
传统的预训练任务采用的是LM，即通过大量无监督的语料进行训练，通过遮蔽下文，将上文输入模型，进行预测即可。But，这种LM预训练任务有一个极大的缺陷：其直接拟合的是从左往右的语义信息，忽略了双向语义信息（因为我们会mask掉下一个token，让当前token去预测下一个token）。

为了拟合双向语义关系，BERT提出了MLM的训练方式。思路如下：在一个文本序列中，我们随机掩盖掉部分的token，让所有没有被遮蔽的token输入模型，要求模型输入预测被遮蔽的token。例如：

```plain
输入：I <MASK> you because you are <MASK>
输出：<MASK> - love; <MASK> - wonderful
```

该预训练任务虽然带来了双向语义的拟合，但却导致预训练任务和下游微调任务发生了冲突，极大程度上会影响下游任务微调的能力。So，how to deal this problem？

**solution**：保留LM同时增加部分mask的语料。具体而言：

在具体进行 MLM 训练时，会随机选择训练语料中 15% 的 token 用于遮蔽。但是这 15% 的 token 并非全部被遮蔽为 `<<font style="color:#DF2A3F;">MASK>`，而是有 80% 的概率被遮蔽，10% 的概率被替换为任意一个 token，还有 10% 的概率保持不变。其中 10% 保持不变就是为了消除预训练和微调的不一致，而 10% 的随机替换核心意义在于迫使模型保持对上下文信息的学习。

#### NSP（Next Sentence Prediction）
NSP 的核心思想是针对句级的 NLU 任务，例如问答匹配、自然语言推理等。通过要求模型判断句对关系，从而迫使模型拟合句子之间的关系，来适配句级的 NLU 任务。

### 微调
所谓微调，其实和训练时更新模型参数的策略一致，只不过在特定的任务、更少的训练数据、更小的 batch_size 上进行训练，更新参数的幅度更小。对于绝大部分下游任务，都可以直接使用 BERT 的输出。例如，对于文本分类任务，可以直接修改模型结构中的 prediction_heads 最后的分类头即可。对于序列标注等任务，可以集成 BERT 多层的隐含层向量再输出最后的标注结果。对于文本生成任务，也同样可以取 Encoder 的输出直接解码得到最终生成结果。因此，BERT 可以非常高效地应用于多种 NLP 任务。



# Encoder-Decoder PLM
Encoder-Decoder模型通过引入Decoder来解决MLM和下游微调不一致的问题。

## T5（Text-To-Text Transfer Transformer）
T5（Text-To-Text Transfer Transformer）是由 Google 提出的一种预训练语言模型，通过将所有 NLP 任务统一表示为文本到文本的转换问题，大大简化了模型设计和任务处理。

![](https://cdn.nlark.com/yuque/0/2025/png/35251293/1764683709357-3ca747f3-7b9e-4400-9288-e15ebf40b5bc.png)

T5模型结构

## 预训练任务
T5的预训练任务主要包括以下几个部分：

+ T5模型的预训练任务是 MLM，也称为BERT-style目标。具体来说，就是在输入文本中随机遮蔽15%的token，然后让模型预测这些被遮蔽的token。这个过程不需要标签，可以在大量未标注的文本上进行。
+ 输入格式: 预训练时，T5将输入文本转换为"文本到文本"的格式。对于一个给定的文本序列，随机选择一些token进行遮蔽，并用特殊的占位符(token)替换。然后将被遮蔽的token序列作为模型的输出目标。
+ 多任务预训练: T5 还尝试了将多个任务混合在一起进行预训练，而不仅仅是单独的MLM任务。这有助于模型学习更通用的语言表示。



# Decoder-Only PLM
仅有Transformer的Decoder部分进行搭建而成的模型。

## GPT
Generative Pretrainning Language Model。

![](https://cdn.nlark.com/yuque/0/2025/png/35251293/1764687508776-c22b7af8-6db8-4af3-8c37-91382102b654.png)

GPT模型结构

比较简单，相较于transformer，GPT采用的decoder部分只是用了单层注意力机制，Decoder 层仅保留了一个带掩码的注意力层，并且将 LayerNorm 层从 Transformer 的注意力层之后提到了注意力层之前。

另外一个结构上的区别在于，GPT 的 MLP 层没有选择线性矩阵来进行特征提取，而是选择了两个一维卷积核来提取，不过，从效果上说这两者是没有太大区别的。通过 N 个 Decoder 层后的 hidden_states 最后经过线性矩阵映射到词表维度，就可以转化成自然语言的 token，从而生成我们的目标序列。

## 预训练任务-CLM
因果语言训练模型，Casual Language Model，下简称 CLM。CLM 则是基于一个自然语言序列的前面所有 token 来预测下一个 token。

## GPT的贡献
GPT2 提出了zero-shot的思路。但是在GPT-2的时代，模型能力还不足之城较好的zero-shot效果，在大模型时代，zero-shot及其延伸的few-shot（少样本学习）才开始逐渐成为主流。

### zero-shot
模型在推理时** 没有提供任何示例**，只给任务指令。模型必须依靠**预训练能力**完成任务。

**举例**

**你问模型：**

```css
Translate to English: 我喜欢苹果
```

**模型直接输出:**

```css
I like apples.
```

**你没有给任何示例（例题）。**

这就是 Zero-shot。



Zero-shot 能力强说明：

+ 模型的指令理解能力强（Instruction following）
+ 预训练知识丰富
+ 泛化能力强

### few-shot
few-shot 是对 zero-shot 的一个折中，旨在提供给模型少样的示例来教会它完成任务。few-shot推理时给模型 **少量示例（比如 1～5 个）**，让模型模仿示例的模式。

**举例：**

```css
Translate Chinese to English:
Input: 我喜欢你
Output: I like you.

Input: 北京今天下雨
Output: It is raining today in Beijing.

Input: 我明天去上海
Output: 
```

**模型看到你给的格式，也按格式翻译**：

```css
I will go to Shanghai tomorrow.
```

**few-shot** 也叫做上下文学习能力（In-context Learning）。  


## LLaMA
**Large Language model Meta AI**。

### 模型架构
![](https://cdn.nlark.com/yuque/0/2025/png/35251293/1764762231989-825155a1-21ec-49c5-b7ea-25ca36218b06.png)

LLaMA模型架构

## GLM
GLM系列是智谱开发的主流中文LLM之一。

### 模型架构-相较于GPT的修改
GLM 最初是由清华计算机系推出的一种通用语言模型基座，其核心思路是在传统 CLM 预训练任务基础上，加入 MLM 思想，从而构建一个在 NLG 和 NLU 任务上都具有良好表现的统一模型。

在整体模型结构上，GLM 和 GPT 大致类似，均是 Decoder-Only 的结构，仅有三点细微差异：

+ 使用Post Norm而非Pre Norm。
+ 使用单层线性层完成Token预测，而不是MLP，减少参数量
+ 激活函数使用GeLUS。

### 预训练任务
GLM 的核心创新点主要在于其提出的 GLM（General Language Model，通用语言模型）任务。GLM 是一种结合了自编码思想和自回归思想的预训练方法。所谓自编码思想，其实也就是 MLM 的任务学习思路，在输入文本中随机删除连续的 tokens，要求模型学习被删除的 tokens；所谓自回归思想，其实就是传统的 CLM 任务学习思路，也就是要求模型按顺序重建连续 tokens。

GLM将MLM和CLM思想进行结合。其核心思想是，对于一个输入序列，会类似于 MLM 一样进行随机的掩码，但遮蔽的不是和 MLM 一样的单个 token，而是每次遮蔽一连串 token；模型在学习时，既需要使用遮蔽部分的上下文预测遮蔽部分，在遮蔽部分内部又需要以 CLM 的方式完成被遮蔽的 tokens 的预测。例如，输入和输出可能是：

```css
输入：I <MASK> because you <MASK>
输出：<MASK> - love you; <MASK> - are a wonderful person

```

