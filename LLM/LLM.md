# 定义
LLM，  Large Language Model。就是参数量大、训练预料大的Language Model。





# LLM的能力
## 涌现能力（Emergent Abilities）
该能力指的是：在同样的模型架构、同样的预训练任务的条件下。某些能力在小模型中表现不明显，但是在大模型中却十分突出。（量变引起质变）。



## 上下文学习能力（In- context Learning）
由GPT3首次提出，也被称作`**Few-Shot**`能力。即在推理阶段，通过给予一些任务示例或自然语言指令，理解上下文并生成对应输出的方式来执行任务。

该能力也诞生了新的关键词：Prompt Engineering，即通过调整Prompt来激发LLM的能力。



## 指令遵循（Instruction Following）
微调阶段，使用自然语言描述的多任务数据集进行微调。即采用的数据集是由自然语言描述而成的任务。该部分主要的工作是：`**让模型按照输入进行回答，避免他答非所问，有了这个能力之后，上下文学习能力才能更好的被激活**`。



## 逐步推理能力（Step by Step Reasoning）
LLM采用的是(Chain-of-Thought, CoT)推理策略。

### Chain-of-Thought（CoT）是什么？
CoT = 思维链 = 在回答之前，把推理步骤写出来的能力。

### CoT在LLM里是如何训练得到的？
#### （1）训练阶段，在训练数据里加入带“推理步骤”的数据
比如：

> **Q: …  
****A: Step 1… Step 2… Step 3… Final Answer…**
>

模型看到大量示例-> 学会“写推理过程的风格”



#### （2）推理阶段，使用promt进行触发
比如让模型：

+ “Let's think step by step.”（经典）
+ “Please show your chain of thought.”
+ “先分析，再给答案。”

这些提示能触发模型输出推理链。

# LLM常说的问题
## 幻觉
由于LLM是通过casual Language Model的预训练任务进行学习，因此不论我们输入什么给模型，模型总会通过softmax输出下一个token，这就导致有时候他的知识并不能回答某个问题时，也会输出看似合理的答案，而这些答案其实是他自己“编的”。

## 如何削弱幻觉
### RAG
Retrieval-Augmented Generation，检索增强生成。

> **先从模型外部的知识库中检索相关内容，然后拼接到输入中，让模型生成答案。**
>



### Prompt限制
通过上下文学习能力，使用promt限制他的回答。



# 训练一个LLM
训练一个完整的 LLM 需要经过三个阶段——Pretrain、SFT 和 RLHF。如下图所示：

![](https://cdn.nlark.com/yuque/0/2025/png/35251293/1765073761907-17323f9e-c739-4521-ae58-98431fe92f9e.png)

训练LLM的三阶段

## Pretrain
前面已经讲过，这里不再重复赘述。此阶段主流LLM都采用Decoder-Only的类GPT架构（LLaMA架构），在Causual Language的预训练任务上进行。

需要补充的是：LLM所采用的参数量远比传统PLM大，所需要的数据也多。因此会有一下问题产生：

+ 多大的数据量才算合适？
+ 如此庞大的计算开销，需要怎么才可以完成？

### Scaling Law
OpenAI提出的`**Scaling Law**`，训练所需要的Token数（数据集的大小）应该是模型参数的`**1.7**`倍。

LLaMA提出，需要使用`**20倍**`的Token才能使模型达到效果最优。



### 分布式训练
如此庞大的参数以及计算量，在单一显卡上进行训练是无法满足的。因此，分布式训练框架在LLM训练中必不可少。而分布式训练框架的核心思想主要从两方面入手：

+ 数据并行
+ 模型并行



#### (1) 数据并行
简单理解，将`**模型实例**`放到多个显卡中，而不同显卡运行不同**Batch**的数据，没完成一次**forward**，收集所`**有实例**`的`**梯度**`，并计算梯度更新，再将更新之后的参数`**传递到所有实例**`。

![](https://cdn.nlark.com/yuque/0/2025/png/35251293/1765074532746-21a2e858-20b6-4ac0-b839-e970fea909f4.png)

数据并行

But，这一切的前提是：`**一个GPU可以完整的放下整个模型**`。  
可实际中，有的LLM参数量特别大，GPU根本无法放下一个完整的模型。How about this one？

#### (2) 模型并行
我们可以把模型拆分为多个部分，将不同部分放入到不同的GPU中进行训练，实现模型并行。

![](https://cdn.nlark.com/yuque/0/2025/png/35251293/1765074896256-92f898d1-cf9c-4a63-8666-4d28ec14383d.png)

#### (3) ZeRO（Zero Redundancy Optimizer）
零冗余优化器。其本质是`**显存优化**`的`**数据并行**`方案，既不是单纯的模型并行，也不是单纯的数据并行。其`**核心思想**`是优化**数据并行**时**每张卡的显存占用**，从而实现对更大规模模型的支持。ZeRO 将模型训练阶段每张卡被占用的显存分为两类：

+ 模型状态（Model States），包括模型参数、模型梯度、优化器Adam的状态参数。假设模型参数量为1M，在混合精度的训练情况下，该部分需要16M的空间进行存储，其中Adma状态参数会占据12M的空间。

> 对于混合训练的情况，
>
>     模型静态显存总需求：16*模型参数量
>
>     Adam优化器部分（含主权重）：约12*模型参数（占总量75%）
>
>     模型参数与梯度：约4*模型参数
>
> 具体分析：
>
>     混合精度训练过程中，显存主要被分为三部分：`**模型参数**`、`**梯度**`、`**优化器的状态**`。
>
>     假设模型参数为1M。
>
> 1、模型参数
>
>     在混合精度训练的forward阶段，model weight使用FP16或BF16存储和计算。
>
>         总显存开销：1M * 2B = 2MB
>
> 2、梯度
>
>     反向传播的梯度也使用FP16或BF16存储（与参数精度一致）
>
>         总显存开销：1M * 2B = 2MB
>
> 3、Adma优化器（FP32）
>
>     在进行参数更新的时候，采用FP32进行，因为参数更新是对模型进行优化，我们认为这个阶段是最重要的，所以采用32位进行存储。而adma优化器需要维护以下数据：
>
>         模型主权重备份（Master weights）：FP32的副本，和forward不同。
>
>         Momentum（m）：一阶动量，FP32
>
>         Variance（v）：二阶动量，FP32
>
>         总显存开销：1M * 4B * 3 = 12MB
>

+ 剩余状态（Residual States），除了模型状态之外的显存占用，包括激活值、各种缓存和显存碎片。

针对上述的显存占用，ZeRO提出了三种不断递进的**优化策略**：

1. ZeRO-1：优化`**Adam状态参数**`进行分片，即每张卡只存储$ \frac{1}{N} $的`**Adam状态参**`数，其余参数保持每张卡一份。
2. ZeRO-2：在ZeRO-1的基础上，进一步优化`**模型梯度**`，即每张卡只存储$ \frac{1}{N} $的`**Adam状态参数**`和`**模型梯度**`，模型参数保持每张卡一份。
3. ZeRO-3：将`**模型参数**`也进行优化，即每张卡只存储$ \frac{1}{N} $的`**Adam状态参数**`、`**模型梯度**`**、**`**模型参数**`**。**

### 分布式训练框架
#### （1）Deepspeed
Deepspeed的核心策略是：**ZeRO**和**CPU-offload**。补充一下`**CPU-off load**`：

它的核心思想是：**利用系统内存来扩展 GPU 显存=的容量**。

简单来讲，就是GPU 显存装不下时，**CPU Offload** 允许将**暂时不用的数据**从 GPU 搬运到 CPU 内存中存储，等需要计算时再搬回 GPU

### 预训练数据集配比
LLM 所掌握的知识绝大部分都是在预训练过程中学会的，因此，为了使训练出的 LLM 能够覆盖尽可能广的知识面，预训练语料需要组织多种来源的数据，并以`一定**比例进行混合**`。

---

## SFT
Supervised Fine-Tuning， 有监督微调。预训练赋予了 LLM 能力，SFT是激发其能力的步骤，让其完成`**指令遵循**`任务--`**指令微调**`，避免答非所问。

所谓`**指令微调**`，即我们训练的输入是各种类型的用户指令，而需要模型拟合的输出则是我们希望模型在收到该指令后做出的回复。例如，我们的一条训练样本可以是：

```css
input:告诉我今天的天气预报？
output:根据天气预报，今天天气是晴转多云，最高温度26摄氏度，最低温度9摄氏度，昼夜温差大，请注意保暖哦
```

也就是说，SFT 的`**主要目标**`是让模型从多种类型、多种风格的指令中获得`**泛化的指令遵循能力**`，也就是能够理解并回复用户的指令。因此，类似于 Pretrain，SFT 的**数据质量**和**数据配比**也是决定模型指令遵循能力的重要因素。

一般 SFT 所使用的指令数据集包括以下三个键：

```css
{
    "instruction":"即输入的用户指令",
    "input":"执行该指令可能需要的补充输入，没有则置空",
    "output":"即模型应该给出的回复"
}
```

例如，如果我们的指令是将目标文本“今天天气真好”翻译成英文，那么该条样本可以构建成如下形式：

```css
{
    "instruction":"将下列文本翻译成英文：",
    "input":"今天天气真好",
    "output":"Today is a nice day！"
}
```

同时，为使模型能够学习到和预训练不同的范式，在 SFT 的过程中，往往会针对性设置特定格式。例如，LLaMA 的 SFT 格式为：

```css
### Instruction:\n{{content}}\n\n### Response:\n
```

其中的 content 即为具体的用户指令，也就是说，对于每一个用户指令，将会嵌入到上文的 content 部分，这里的用户指令不仅指上例中的 “instruction”，而是指令和输入的拼接，即模型可以执行的一条完整指令。例如，针对上例，LLaMA 获得的输入应该是：

```css
### Instruction:\n将下列文本翻译成英文：今天天气真好\n\n### Response:\n
```

其需要拟合的输出则是：

```css
### Instruction:\n将下列文本翻译成英文：今天天气真好\n\n### Response:\nToday is a nice day！
```

注意，因为指令微调本质上仍然是对模型进行 CLM 训练，只不过要求模型对指令进行理解和回复而不是简单地预测下一个 token，所以模型预测的结果不仅是 output，而应该是 input + output，只不过 input 部分不参与 loss 的计算，但回复指令本身还是以预测下一个 token 的形式来实现的。

## RLHF
Reinforcement Learning from Human Feedback，人类反馈强化学习。利用强化学习来训练LLM。

从功能上出发，LLM的训练可以分为：预训练+对齐（alignment）两个阶段。`**预训练**`的核心在于**赋予模型海量**的知识，而`**对齐**`就是让模型能和人类**正常交互**，同时保证模型的回答符合人类的预期（答案是否正确、内容是否与我们的价值观相同）。

在这个过程中，SFT是让LLM与人类的指令对齐，从而具有指令遵循能力。

![](https://cdn.nlark.com/yuque/0/2025/png/35251293/1765083136447-1249bde1-da60-4ab8-b247-6bb6f6d8c858.png)

ChatGPT对齐训练的三个阶段

RLHF 的思路是，引入强化学习的技术，通过实时的人类反馈令 LLM 能够给出更令人类满意的回复。

---

RM，Reward Model，即`**奖励模型**`。RM 是用于拟合人类偏好，来给 LLM 做出反馈的。在强化学习的训练中，对于 LLM 的每一个回复，RM 会进行打分，这个打分反映了生成回复符合人类偏好的程度。然后 LLM 会根据强化学习的原理，基于 RM 的打分来进行优化训练。所以，RM 本质上是一个`**文本分类模型**`，对于一个文本输出一个`**标量奖励**`，和文本分类任务中的隐藏层输出非常类似。

But，在世纪训练RM时，由于数据时人工标注的，这个标注者之间存在价值观的差异，因此通过文本直接给予标量奖励往往是存在巨大差异的。为此，我们是采用对一个对话，不同回复进行`**排序**`，再将`**排序转化为奖励**`。

---

PPO，Proximal Policy Optimization。近端策略优化算法，是一种经典的 RL 算法。

在具体 PPO 训练过程中，会存在四个模型。如图4.5所示，两个 LLM 和两个 RM。两个 LLM 分别是进行微调、参数更新的 actor model 和不进行参数更新的 ref model，均是从 SFT 之后的 LLM 初始化的。两个 RM 分别是进行参数更新的 critic model 和不进行参数更新的 reward model，均是从上一步训练的 RM 初始化的

![](https://cdn.nlark.com/yuque/0/2025/png/35251293/1765083864949-c0d9275e-2d3e-4552-9c79-82ce57319e14.png)

PPO训练流程



